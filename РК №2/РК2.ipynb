{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "РК2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-TZDVjPPjYS"
      },
      "source": [
        "# Рубежный контроль №2 по курсу \"Методы машинного обучения\"  \n",
        "**Чжан Чжибо  ИУ5И-21М**  \n",
        "Вариант №1: CountVectorizer, TfidfVectorizer, LogisticRegression, Multinomial Naive Bayes (MNB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02tmCtfCRYxe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline \n",
        "sns.set(style=\"ticks\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa44-iSMSgEQ"
      },
      "source": [
        "def accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Вычисление метрики accuracy для каждого класса\n",
        "    y_true - истинные значения классов\n",
        "    y_pred - предсказанные значения классов\n",
        "    Возвращает словарь: ключ - метка класса, \n",
        "    значение - Accuracy для данного класса\n",
        "    \"\"\"\n",
        "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
        "    d = {'t': y_true, 'p': y_pred}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    # Метки классов\n",
        "    classes = np.unique(y_true)\n",
        "    # Результирующий словарь\n",
        "    res = dict()\n",
        "    # Перебор меток классов\n",
        "    for c in classes:\n",
        "        # отфильтруем данные, которые соответствуют \n",
        "        # текущей метке класса в истинных значениях\n",
        "        temp_data_flt = df[df['t']==c]\n",
        "        # расчет accuracy для заданной метки класса\n",
        "        temp_acc = accuracy_score(\n",
        "            temp_data_flt['t'].values, \n",
        "            temp_data_flt['p'].values)\n",
        "        # сохранение результата в словарь\n",
        "        res[c] = temp_acc\n",
        "    return res\n",
        "\n",
        "def print_accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Вывод метрики accuracy для каждого класса\n",
        "    \"\"\"\n",
        "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
        "    if len(accs)>0:\n",
        "        print('Метка \\t Accuracy')\n",
        "    for i in accs:\n",
        "        print('{} \\t {}'.format(i, accs[i]))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4DVfg0JU6aq"
      },
      "source": [
        "Загрузка данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KwNIOh-VGVD",
        "outputId": "14212304-81f6-45fa-ce4b-24bf228501af"
      },
      "source": [
        "%cd /content/drive/MyDrive/dataset/spam/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/dataset/spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Yff4cI35Uz2C",
        "outputId": "7cdc2a42-18ac-4e1f-8bd9-7aa8b7bae5d1"
      },
      "source": [
        "dataset = pd.read_csv(\"enron_spam_data.csv\")\n",
        "dataset.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>christmas tree farm pictures</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>vastar resources , inc .</td>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>calpine daily gas nomination</td>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>re : issue</td>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>meter 7268 nov allocation</td>\n",
              "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                       Subject  ... Spam/Ham        Date\n",
              "0           0  christmas tree farm pictures  ...      ham  1999-12-10\n",
              "1           1      vastar resources , inc .  ...      ham  1999-12-13\n",
              "2           2  calpine daily gas nomination  ...      ham  1999-12-14\n",
              "3           3                    re : issue  ...      ham  1999-12-14\n",
              "4           4     meter 7268 nov allocation  ...      ham  1999-12-14\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "_gheYh1eVbRj",
        "outputId": "dcfdefe5-1280-48e5-958d-53eab5bac09a"
      },
      "source": [
        "dataset=dataset.drop(['Unnamed: 0','Subject','Date'],axis=1)\n",
        "dataset.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Message Spam/Ham\n",
              "0                                                NaN      ham\n",
              "1  gary , production from the high island larger ...      ham\n",
              "2             - calpine daily gas nomination 1 . doc      ham\n",
              "3  fyi - see note below - already done .\\nstella\\...      ham\n",
              "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "LQaNrCerW8Vb",
        "outputId": "6742cb84-5f79-42be-9ad6-1c5035b84bd9"
      },
      "source": [
        "dataset['Spam/Ham']=dataset['Spam/Ham'].replace(['ham','spam'],[0,1])\n",
        "dataset.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Message  Spam/Ham\n",
              "0                                                NaN         0\n",
              "1  gary , production from the high island larger ...         0\n",
              "2             - calpine daily gas nomination 1 . doc         0\n",
              "3  fyi - see note below - already done .\\nstella\\...         0\n",
              "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "p3wLtCdBYZK0",
        "outputId": "2a7ec37e-6359-4397-98eb-11ba65101c01"
      },
      "source": [
        "dataset=dataset.dropna()\n",
        "dataset.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>jackie ,\\nsince the inlet to 3 river plant is ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Message  Spam/Ham\n",
              "1  gary , production from the high island larger ...         0\n",
              "2             - calpine daily gas nomination 1 . doc         0\n",
              "3  fyi - see note below - already done .\\nstella\\...         0\n",
              "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...         0\n",
              "5  jackie ,\\nsince the inlet to 3 river plant is ...         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqhrhjmWoHs"
      },
      "source": [
        "Сформируем общий словарь для обучения моделей из обучающей и тестовой выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTd6cO6bWJ9h",
        "outputId": "beb692db-72af-44bd-c015-33c177d0a6a1"
      },
      "source": [
        "vocab_list = dataset['Message'].tolist()\n",
        "vocab_list[1:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['- calpine daily gas nomination 1 . doc',\n",
              " 'fyi - see note below - already done .\\nstella\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by stella l morris / hou / ect on 12 / 14 / 99 10 : 18\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nfrom : sherlyn schumack on 12 / 14 / 99 10 : 06 am\\nto : stella l morris / hou / ect @ ect\\ncc : howard b camp / hou / ect @ ect\\nsubject : re : issue\\nstella ,\\nthis has already been taken care of . you did this for me yesterday .\\nthanks .\\nhoward b camp\\n12 / 14 / 99 09 : 10 am\\nto : stella l morris / hou / ect @ ect\\ncc : sherlyn schumack / hou / ect @ ect , howard b camp / hou / ect @ ect , stacey\\nneuweiler / hou / ect @ ect , daren j farmer / hou / ect @ ect\\nsubject : issue\\nstella ,\\ncan you work with stacey or daren to resolve\\nhc\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by howard b camp / hou / ect on 12 / 14 / 99 09 : 08\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nfrom : sherlyn schumack 12 / 13 / 99 01 : 14 pm\\nto : howard b camp / hou / ect @ ect\\ncc :\\nsubject : issue\\ni have to create accounting arrangement for purchase from unocal energy at\\nmeter 986782 . deal not tracked for 5 / 99 . volume on deal 114427 expired 4 / 99 .',\n",
              " 'fyi .\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by lauri a allen / hou / ect on 12 / 14 / 99 12 : 17\\npm - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nkimberly vaughn\\n12 / 10 / 99 02 : 54 pm\\nto : lauri a allen / hou / ect @ ect\\ncc : mary m smith / hou / ect @ ect\\nsubject : meter 7268 nov allocation\\nlauri . . i have put this on strangas gas until i can get a contract from\\ndaren .\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by kimberly vaughn / hou / ect on 12 / 10 / 99 01 : 52\\npm - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nlauri a allen\\n12 / 09 / 99 01 : 20 pm\\nto : kimberly vaughn / hou / ect @ ect , anita luong / hou / ect @ ect\\ncc : howard b camp / hou / ect @ ect , mary m smith / hou / ect @ ect\\nsubject : meter 7268 nov allocation\\nkim / anita -\\na volume of 7247 mm shows to have been allocated to the reliant 201 contract\\nfor november . there was no nomination for reliant at this point in november\\nand , therefore , there should be no volume allocated to their contract .\\nplease make sure these volumes are moved off the reliant contract prior to\\nnovember close .\\nthanks .',\n",
              " 'jackie ,\\nsince the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\\nflow ) :\\nat what meter is the mcmullen gas being diverted to ?\\nat what meter is hpl buying the residue gas ? ( this is the gas from teco ,\\nvastar , vintage , tejones , and swift )\\ni still see active deals at meter 3405 in path manager for teco , vastar ,\\nvintage , tejones , and swift\\ni also see gas scheduled in pops at meter 3404 and 3405 .\\nplease advice . we need to resolve this as soon as possible so settlement\\ncan send out payments .\\nthanks',\n",
              " 'george ,\\ni need the following done :\\njan 13\\nzero out 012 - 27049 - 02 - 001 receipt package id 2666\\nallocate flow of 149 to 012 - 64610 - 02 - 055 deliv package id 392\\njan 26\\nzero out 012 - 27049 - 02 - 001 receipt package id 3011\\nzero out 012 - 64610 - 02 - 055 deliv package id 392\\nthese were buybacks that were incorrectly nominated to transport contracts\\n( ect 201 receipt )\\nlet me know when this is done\\nhc',\n",
              " \"fyi\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by gary l payne / hou / ect on 12 / 14 / 99 02 : 35 pm\\n- - - - - - - - - - - - - - - - - - - - - - - - - - -\\nfrom : antoine v pierre 12 / 14 / 99 02 : 34 pm\\nto : tommy j yanowski / hou / ect @ ect , kathryn bussell / hou / ect @ ect , gary l\\npayne / hou / ect @ ect , diane e niestrath / hou / ect @ ect , romeo d ' souza / hou / ect @ ect ,\\nmichael eiben / hou / ect @ ect , clem cernosek / hou / ect @ ect , scotty\\ngilbert / hou / ect @ ect , dave nommensen / hou / ect @ ect , david rohan / hou / ect @ ect ,\\nkevin heal / cal / ect @ ect , richard pinion / hou / ect @ ect\\ncc : mary g gosnell / hou / ect @ ect , jason moore / hou / ect @ ect , samuel\\nschott / hou / ect @ ect , bernice rodriguez / hou / ect @ ect\\nsubject : duns number changes\\ni will be making these changes at 11 : 00 am on wednesday december 15 .\\nif you do not agree or have a problem with the dnb number change please\\nnotify me , otherwise i will make the change as scheduled .\\ndunns number change :\\ncounterparty cp id number\\nfrom to\\ncinergy resources inc . 62163 869279893 928976257\\nenergy dynamics management , inc . 69545 825854664 088889774\\nsouth jersey resources group llc 52109 789118270 036474336\\ntransalta energy marketing ( us ) inc . 62413 252050406 255326837\\nphiladelphia gas works 33282 148415904 146907159\\nthanks ,\\nrennie\\n3 - 7578\",\n",
              " 'there are two fields of gas that i am having difficulty with in the unify\\nsystem .\\n1 . cage ranch - since there is no processing agreement that accomodates this\\ngas on king ranch , it is my understanding hpl is selling the liquids and\\nking ranch is re - delivering to stratton . it is also my understanding that\\nthere is a . 05 cent fee\\nto deliver this gas . we need a method to accomodate the volume flow on hpl\\nat meter 415 and 9643 . this gas\\nwill not be reflected on trans . usage ticket # 123395 and # 95394 since it is\\nnot being nominated from a processing agreement . we either , need to input\\na point nom ( on hpl or krgp ) at these meters to match the nom at meter 9610 ,\\nor a deal for purchase and sale ( if king ranch is taking title to the gas )\\nneeds to be input into sitara at these meters with the appropriate rate . i\\nhave currently input a point nom on krgp to accomodate this flow , so we can\\ndivert some of this gas to the current interstate sales that are being made .\\n2 . forest oil - there is a processing agreement that will accomodate flow\\nfrom the meter ( 6396 ) into king ranch . it is my\\nunderstanding that this agreement was originally setup until texaco had\\ntheir own processing agreement . i need confirmation that the gas from this\\nmeter should be nominated on contract # ( 96006681 ) and that this agreement\\nshould have been reassigned to hplc . ( it is currently still under hplr ) .\\nif this gas is not nominated on the above transport agreement , then once\\nagain we need to accomodate the flow volume on the hpl pipe with either a\\npoint nom or a sitara deal at meters 415 and 9643 .',\n",
              " 'thanks so much for the memo . i would like to reiterate my support on two key\\nissues :\\n1 ) . thu - best of luck on this new assignment . howard has worked hard and\\ndone a great job ! please don \\' t be shy on asking questions . entex is\\ncritical to the texas business , and it is critical to our team that we are\\ntimely and accurate .\\n2 ) . rita : thanks for setting up the account team . communication is\\ncritical to our success , and i encourage you all to keep each other informed\\nat all times . the p & l impact to our business can be significant .\\nadditionally , this is high profile , so we want to assure top quality .\\nthanks to all of you for all of your efforts . let me know if there is\\nanything i can do to help provide any additional support .\\nrita wynne\\n12 / 14 / 99 02 : 38 : 45 pm\\nto : janet h wallis / hou / ect @ ect , ami chokshi / corp / enron @ enron , howard b\\ncamp / hou / ect @ ect , thu nguyen / hou / ect @ ect , kyle r lilly / hou / ect @ ect , stacey\\nneuweiler / hou / ect @ ect , george grant / hou / ect @ ect , julie meyers / hou / ect @ ect\\ncc : daren j farmer / hou / ect @ ect , kathryn cordes / hou / ect @ ect , rita\\nwynne / hou / ect , lisa csikos / hou / ect @ ect , brenda f herod / hou / ect @ ect , pamela\\nchambers / corp / enron @ enron\\nsubject : entex transistion\\nthe purpose of the email is to recap the kickoff meeting held on yesterday\\nwith members from commercial and volume managment concernig the entex account :\\neffective january 2000 , thu nguyen ( x 37159 ) in the volume managment group ,\\nwill take over the responsibility of allocating the entex contracts . howard\\nand thu began some training this month and will continue to transition the\\naccount over the next few months . entex will be thu \\' s primary account\\nespecially during these first few months as she learns the allocations\\nprocess and the contracts .\\nhoward will continue with his lead responsibilites within the group and be\\navailable for questions or as a backup , if necessary ( thanks howard for all\\nyour hard work on the account this year ! ) .\\nin the initial phases of this transistion , i would like to organize an entex\\n\" account \" team . the team ( members from front office to back office ) would\\nmeet at some point in the month to discuss any issues relating to the\\nscheduling , allocations , settlements , contracts , deals , etc . this hopefully\\nwill give each of you a chance to not only identify and resolve issues before\\nthe finalization process , but to learn from each other relative to your\\nrespective areas and allow the newcomers to get up to speed on the account as\\nwell . i would encourage everyone to attend these meetings initially as i\\nbelieve this is a critical part to the success of the entex account .\\ni will have my assistant to coordinate the initial meeting for early 1 / 2000 .\\nif anyone has any questions or concerns , please feel free to call me or stop\\nby . thanks in advance for everyone \\' s cooperation . . . . . . . . . . .\\njulie - please add thu to the confirmations distributions list',\n",
              " 'the purpose of the email is to recap the kickoff meeting held on yesterday\\nwith members from commercial and volume managment concernig the entex account :\\neffective january 2000 , thu nguyen ( x 37159 ) in the volume managment group ,\\nwill take over the responsibility of allocating the entex contracts . howard\\nand thu began some training this month and will continue to transition the\\naccount over the next few months . entex will be thu \\' s primary account\\nespecially during these first few months as she learns the allocations\\nprocess and the contracts .\\nhoward will continue with his lead responsibilites within the group and be\\navailable for questions or as a backup , if necessary ( thanks howard for all\\nyour hard work on the account this year ! ) .\\nin the initial phases of this transistion , i would like to organize an entex\\n\" account \" team . the team ( members from front office to back office ) would\\nmeet at some point in the month to discuss any issues relating to the\\nscheduling , allocations , settlements , contracts , deals , etc . this hopefully\\nwill give each of you a chance to not only identify and resolve issues before\\nthe finalization process , but to learn from each other relative to your\\nrespective areas and allow the newcomers to get up to speed on the account as\\nwell . i would encourage everyone to attend these meetings initially as i\\nbelieve this is a critical part to the success of the entex account .\\ni will have my assistant to coordinate the initial meeting for early 1 / 2000 .\\nif anyone has any questions or concerns , please feel free to call me or stop\\nby . thanks in advance for everyone \\' s cooperation . . . . . . . . . . .\\njulie - please add thu to the confirmations distributions list']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7ApdFLRZ0ON"
      },
      "source": [
        "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
        "    for v in vectorizers_list:\n",
        "        for c in classifiers_list:\n",
        "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
        "            score = cross_val_score(pipeline1, dataset['Message'], dataset['Spam/Ham'], scoring='accuracy', cv=3).mean()\n",
        "            print('Векторизация - {}'.format(v))\n",
        "            print('Модель для классификации - {}'.format(c))\n",
        "            print('Accuracy = {}'.format(score))\n",
        "            print('===========================')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GbVPYr5eMXN",
        "outputId": "1cc48b59-f900-4303-ab79-0dcbee9b11ef"
      },
      "source": [
        "vocabVect = CountVectorizer()\n",
        "vocabVect.fit(vocab_list)\n",
        "corpusVocab = vocabVect.vocabulary_\n",
        "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество сформированных признаков - 60049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpkJFXieZ3b5",
        "outputId": "f3392ddc-8d8b-4f96-d796-1c89e95f2986"
      },
      "source": [
        "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
        "classifiers_list = [LogisticRegression(), MultinomialNB()]\n",
        "VectorizeAndClassify(vectorizers_list, classifiers_list)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None,\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '000000': 3,\n",
            "                            '00...\n",
            "                            '000000000005412': 12, '000000000005413': 13,\n",
            "                            '000000000005820': 14, '000000000006238': 15,\n",
            "                            '000000000006452': 16, '000000000007399': 17,\n",
            "                            '000000000007494': 18, '000000000007498': 19,\n",
            "                            '000000000007568': 20, '000000000007588': 21,\n",
            "                            '000000000007589': 22, '000000000007590': 23,\n",
            "                            '000000000007591': 24, '000000000007592': 25,\n",
            "                            '000000000007593': 26, '000000000007666': 27,\n",
            "                            '000000000007874': 28, '000000000007876': 29, ...})\n",
            "Модель для классификации - LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Accuracy = 0.7290913088764381\n",
            "===========================\n",
            "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None,\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '000000': 3,\n",
            "                            '00...\n",
            "                            '000000000005412': 12, '000000000005413': 13,\n",
            "                            '000000000005820': 14, '000000000006238': 15,\n",
            "                            '000000000006452': 16, '000000000007399': 17,\n",
            "                            '000000000007494': 18, '000000000007498': 19,\n",
            "                            '000000000007568': 20, '000000000007588': 21,\n",
            "                            '000000000007589': 22, '000000000007590': 23,\n",
            "                            '000000000007591': 24, '000000000007592': 25,\n",
            "                            '000000000007593': 26, '000000000007666': 27,\n",
            "                            '000000000007874': 28, '000000000007876': 29, ...})\n",
            "Модель для классификации - MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
            "Accuracy = 0.7107923073877237\n",
            "===========================\n",
            "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use...\n",
            "                            '000000000005412': 12, '000000000005413': 13,\n",
            "                            '000000000005820': 14, '000000000006238': 15,\n",
            "                            '000000000006452': 16, '000000000007399': 17,\n",
            "                            '000000000007494': 18, '000000000007498': 19,\n",
            "                            '000000000007568': 20, '000000000007588': 21,\n",
            "                            '000000000007589': 22, '000000000007590': 23,\n",
            "                            '000000000007591': 24, '000000000007592': 25,\n",
            "                            '000000000007593': 26, '000000000007666': 27,\n",
            "                            '000000000007874': 28, '000000000007876': 29, ...})\n",
            "Модель для классификации - LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Accuracy = 0.7313786840625275\n",
            "===========================\n",
            "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use...\n",
            "                            '000000000005412': 12, '000000000005413': 13,\n",
            "                            '000000000005820': 14, '000000000006238': 15,\n",
            "                            '000000000006452': 16, '000000000007399': 17,\n",
            "                            '000000000007494': 18, '000000000007498': 19,\n",
            "                            '000000000007568': 20, '000000000007588': 21,\n",
            "                            '000000000007589': 22, '000000000007590': 23,\n",
            "                            '000000000007591': 24, '000000000007592': 25,\n",
            "                            '000000000007593': 26, '000000000007666': 27,\n",
            "                            '000000000007874': 28, '000000000007876': 29, ...})\n",
            "Модель для классификации - MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
            "Accuracy = 0.7113864156174105\n",
            "===========================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxzv76EYgyZu"
      },
      "source": [
        "Лучшую точность показал TfidfVectorizer и LogisticRegression (73,1%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUt8F-9_hTQQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}